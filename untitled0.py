# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cnht31HQerg2-EX2JR1t1Mbi-AkdWXAM
"""

!pip install llama-index

pip install llama-index-llms-groq

from llama_index.llms.groq import Groq

GROQ_API_KEY="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"

pip install  --upgrade llama-index-llms-groq

llm=Groq(model="llama3-70b-8192",api_key="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")

response=llm.complete("explain the importance of low latency")

print(response)

from llama_index.core.llms import ChatMessage

Messages=[ChatMessage(role="system", content="you are a private with a colorful personality"), ChatMessage(role="user", content="what is your name"),]
resp=llm.chat(Messages)

print(resp)

response=llm.stream_complete("explain the importance of low latency LLMS")

for  r in response:
  print(r.delta,end="")

import logging
import sys
from google.colab import userdata
import os
from llama_index.core import VectorStoreIndex, simpleDirectoryReader, Settings
from llama_index.embaddings.huggingface import HuggingFaceEmbedding
#from llama_index.llms.openai import openAI
from llama_index.llms.groq import Groq

import logging
import sys
from google.colab import userdata
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
#from llama_index.llms.openai import openAI
from llama_index.llms.groq import Groq

import logging
import sys
from google.colab import userdata
import os
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
# Install llama-index with huggingface extra
!pip install llama-index[huggingface]
from llama_index.embeddings.huggingface import HuggingFaceEmbedding # This import should now work
#from llama_index.llms.openai import openAI
from llama_index.llms.groq import Groq

GROQ_API_KEY="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"

mkdir data

wget -0 data/eassy.txt https://colab.research.google.com/drive/1s6mvjR1aPB3h2MQOaIdnWttVnc8f-2ey?usp=sharing

index=VectorStoreIndex.from_documents(documents)

index = VectorStoreIndex.from_documents(documents)

